# How Kubernetes Works - Complete Explanation for Interviews

Here's a comprehensive explanation you can use when an interviewer asks "How does Kubernetes work?"

---

## High-Level Answer (30 seconds)

*"Kubernetes is a container orchestration platform that automates the deployment, scaling, and management of containerized applications. It works on a master-worker architecture where the control plane manages the cluster state, and worker nodes run the actual application containers. When you submit a deployment, Kubernetes continuously monitors and maintains your desired state - if a container crashes, it automatically restarts it. It provides features like service discovery, load balancing, automated rollouts, and self-healing."*

---

## Detailed Architecture & Workflow

### 1. **Kubernetes Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONTROL PLANE (Master)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  API Server  â”‚  â”‚  Scheduler   â”‚  â”‚Controller â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  Manager  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              etcd (Cluster State)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  WORKER NODE  â”‚ â”‚ WORKER NODE â”‚ â”‚ WORKER NODE â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Kubelet     â”‚ â”‚   Kubelet   â”‚ â”‚   Kubelet   â”‚
â”‚   Kube-proxy  â”‚ â”‚   Kube-proxyâ”‚ â”‚   Kube-proxyâ”‚
â”‚               â”‚ â”‚             â”‚ â”‚             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Pod (App) â”‚ â”‚ â”‚ â”‚   Pod   â”‚ â”‚ â”‚ â”‚   Pod   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚    Pod    â”‚ â”‚ â”‚ â”‚   Pod   â”‚ â”‚ â”‚ â”‚   Pod   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complete Deployment Workflow (Step-by-Step)

### **Scenario: You run `kubectl apply -f deployment.yaml`**

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: nginx:1.24
        ports:
        - containerPort: 80
```

---

### **Step 1: kubectl â†’ API Server**

```
Developer
    â”‚
    â”‚ kubectl apply -f deployment.yaml
    â–¼
API Server (validates request)
    â”‚
    â”‚ Authentication: Is user authenticated?
    â”‚ Authorization: Does user have permissions?
    â”‚ Admission Control: Does request meet policies?
    â–¼
Request Accepted
```

**What happens:**
- `kubectl` sends HTTP request to API Server
- API Server authenticates you (using kubeconfig)
- Checks if you have permissions (RBAC)
- Validates the YAML syntax and schema
- Runs admission controllers (webhooks, policies)

---

### **Step 2: API Server â†’ etcd (Store State)**

```
API Server
    â”‚
    â”‚ Store deployment spec in etcd
    â–¼
etcd (Cluster Database)
    â”‚
    â”‚ Deployment "webapp" created
    â”‚ Desired State: 3 replicas
    â”‚ Status: Not yet created
    â–¼
State Saved
```

**What happens:**
- API Server writes deployment object to **etcd**
- etcd is a distributed key-value store (cluster's brain)
- Stores **desired state**: "I want 3 pods running nginx"
- All cluster state lives here (deployments, pods, services, etc.)

---

### **Step 3: Deployment Controller Detects Change**

```
Deployment Controller (watches API Server)
    â”‚
    â”‚ "New deployment detected!"
    â”‚ Desired: 3 replicas
    â”‚ Current: 0 replicas
    â”‚ Action needed: Create ReplicaSet
    â–¼
Creates ReplicaSet Object
    â”‚
    â”‚ POST /api/v1/replicasets
    â–¼
API Server â†’ etcd
    â”‚
    â”‚ ReplicaSet "webapp-abc123" created
    â”‚ Desired: 3 pods
    â–¼
Stored in etcd
```

**What happens:**
- **Deployment Controller** runs in Control Plane
- Constantly watches API Server for deployment changes
- Sees new deployment needs 3 replicas
- Creates a **ReplicaSet** object to manage pods
- ReplicaSet spec stored in etcd

---

### **Step 4: ReplicaSet Controller Creates Pods**

```
ReplicaSet Controller (watches API Server)
    â”‚
    â”‚ "New ReplicaSet detected!"
    â”‚ Desired: 3 pods
    â”‚ Current: 0 pods
    â”‚ Action: Create 3 pod objects
    â–¼
Creates 3 Pod Objects
    â”‚
    â”‚ POST /api/v1/pods (webapp-abc123-xyz1)
    â”‚ POST /api/v1/pods (webapp-abc123-xyz2)
    â”‚ POST /api/v1/pods (webapp-abc123-xyz3)
    â–¼
API Server â†’ etcd
    â”‚
    â”‚ 3 Pods created (status: Pending)
    â”‚ No node assigned yet
    â–¼
Stored in etcd
```

**What happens:**
- **ReplicaSet Controller** sees new ReplicaSet
- Creates 3 individual Pod objects
- Pods are in "Pending" state (not running yet)
- No node assigned (that's Scheduler's job)

---

### **Step 5: Scheduler Assigns Pods to Nodes**

```
Scheduler (watches for unscheduled pods)
    â”‚
    â”‚ "Found 3 pending pods!"
    â”‚ Need to assign to nodes
    â–¼
Scheduling Algorithm
    â”‚
    â”œâ”€ Filter Nodes
    â”‚  â”‚ âœ… Node1: Has enough CPU/Memory
    â”‚  â”‚ âœ… Node2: Has enough CPU/Memory
    â”‚  â”‚ âŒ Node3: Insufficient memory
    â”‚
    â”œâ”€ Score Nodes
    â”‚  â”‚ Node1: Score 85 (least loaded)
    â”‚  â”‚ Node2: Score 72 (more loaded)
    â”‚
    â”œâ”€ Binding Decision
    â”‚  â”‚ Pod1 â†’ Node1
    â”‚  â”‚ Pod2 â†’ Node2
    â”‚  â”‚ Pod3 â†’ Node1
    â–¼
Update pods with node assignments
    â”‚
    â”‚ PATCH /api/v1/pods/webapp-abc123-xyz1
    â”‚   spec.nodeName: node1
    â–¼
API Server â†’ etcd
    â”‚
    â”‚ Pods updated with node assignments
    â–¼
Stored
```

**What happens:**
- **Scheduler** watches for pods without node assignment
- Runs two-phase algorithm:
  1. **Filtering**: Eliminates nodes that can't run pod
     - Insufficient resources (CPU/memory)
     - Node selectors don't match
     - Taints/tolerations conflicts
  2. **Scoring**: Ranks suitable nodes
     - Resource balance
     - Pod spreading
     - Affinity rules
- Assigns each pod to best node
- Updates pod objects with `nodeName` field

---

### **Step 6: Kubelet Runs Containers**

```
Kubelet on Node1 (watches API Server)
    â”‚
    â”‚ "I have 2 pods assigned to me!"
    â”‚ Pod1: webapp-abc123-xyz1
    â”‚ Pod3: webapp-abc123-xyz3
    â–¼
For each pod:
    â”‚
    â”œâ”€ Pull Image
    â”‚  â”‚ docker pull nginx:1.24
    â”‚  â”‚ (or containerd/cri-o pull)
    â”‚  â–¼
    â”‚  Image downloaded
    â”‚
    â”œâ”€ Create Container
    â”‚  â”‚ docker run nginx:1.24
    â”‚  â”‚ Apply resource limits
    â”‚  â”‚ Set up networking
    â”‚  â–¼
    â”‚  Container started
    â”‚
    â”œâ”€ Health Checks
    â”‚  â”‚ Run liveness probe
    â”‚  â”‚ Run readiness probe
    â”‚  â–¼
    â”‚  Container healthy
    â”‚
    â”œâ”€ Report Status
    â”‚  â”‚ PATCH /api/v1/pods/webapp-abc123-xyz1
    â”‚  â”‚   status.phase: Running
    â”‚  â”‚   status.containerStatuses[0].ready: true
    â”‚  â–¼
    â”‚  API Server â†’ etcd
    â”‚
    â–¼
Pod Running Successfully!
```

**What happens:**
- **Kubelet** (agent on each node) watches API Server
- Sees pods assigned to its node
- For each pod:
  1. **Pulls container image** from registry
  2. **Creates container** using container runtime (Docker/containerd)
  3. **Sets up networking** (assigns IP, DNS)
  4. **Applies resource limits** (CPU/memory)
  5. **Runs health checks** (liveness/readiness probes)
  6. **Reports status** back to API Server
- Pod transitions: Pending â†’ ContainerCreating â†’ Running

---

### **Step 7: Service Discovery & Networking**

```
Service Object (if you created one)
    â”‚
    â”‚ apiVersion: v1
    â”‚ kind: Service
    â”‚ metadata:
    â”‚   name: webapp-service
    â”‚ spec:
    â”‚   selector:
    â”‚     app: webapp
    â”‚   ports:
    â”‚   - port: 80
    â”‚     targetPort: 80
    â–¼
Kube-proxy (on each node)
    â”‚
    â”‚ Watches Service objects
    â”‚ Creates iptables/IPVS rules
    â”‚
    â”œâ”€ Rule 1: webapp-service:80
    â”‚  â”‚ â†’ LoadBalance to:
    â”‚  â”‚   - Pod1 IP: 10.244.1.5:80
    â”‚  â”‚   - Pod2 IP: 10.244.2.7:80
    â”‚  â”‚   - Pod3 IP: 10.244.1.6:80
    â”‚
    â”œâ”€ DNS Entry
    â”‚  â”‚ webapp-service.default.svc.cluster.local
    â”‚  â”‚ â†’ Resolves to Service ClusterIP
    â”‚
    â–¼
Traffic flows to pods
```

**What happens:**
- **kube-proxy** (runs on each node) watches Services
- Creates **iptables/IPVS rules** for load balancing
- **DNS** entry created automatically
- Any pod can access service by name:
  ```bash
  curl http://webapp-service
  ```
- Traffic distributed across all healthy pods

---

### **Step 8: Continuous Reconciliation (Self-Healing)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Continuous Monitoring Loop      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Controller  â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Manager    â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
           â”‚                       â”‚
           â”œâ”€ Check: Desired = 3 pods
           â”‚         Current = 3 pods
           â”‚         âœ… OK          â”‚
           â”‚                       â”‚
    [Pod crashes]                 â”‚
           â”‚                       â”‚
           â”œâ”€ Check: Desired = 3 pods
           â”‚         Current = 2 pods
           â”‚         âŒ Need action!
           â”‚                       â”‚
           â”œâ”€ Create new pod      â”‚
           â”‚                       â”‚
           â”œâ”€ Scheduler assigns    â”‚
           â”‚                       â”‚
           â”œâ”€ Kubelet starts       â”‚
           â”‚                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  Reconciled!
```

**What happens:**
- Controllers run in **infinite loops** (reconciliation loops)
- Constantly comparing **desired state** vs **current state**
- If mismatch detected:
  - Pod crashed? â†’ Create new pod
  - Too few replicas? â†’ Create more
  - Too many? â†’ Delete excess
  - Wrong image version? â†’ Rolling update
- This is **self-healing** in action

---

## Interview Talking Points

### **1. Control Plane Components**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        CONTROL PLANE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ API Server: Front-end, REST API   â”‚
â”‚ â€¢ etcd: Database, stores state      â”‚
â”‚ â€¢ Scheduler: Assigns pods to nodes  â”‚
â”‚ â€¢ Controller Manager: Reconciliationâ”‚
â”‚ â€¢ Cloud Controller: Cloud integrationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain:**
- **API Server**: "The heart of Kubernetes - all communication goes through it"
- **etcd**: "Distributed database that stores the entire cluster state"
- **Scheduler**: "Decides which node should run each pod based on resources"
- **Controller Manager**: "Runs controllers that maintain desired state - deployment controller, replicaset controller, node controller, etc."

---

### **2. Worker Node Components**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         WORKER NODE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Kubelet: Node agent, runs pods    â”‚
â”‚ â€¢ Kube-proxy: Network proxy         â”‚
â”‚ â€¢ Container Runtime: Docker/containerdâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain:**
- **Kubelet**: "Agent on each node that talks to API Server, pulls images, starts containers"
- **Kube-proxy**: "Handles networking, creates iptables rules for services"
- **Container Runtime**: "Actually runs the containers - Docker, containerd, CRI-O"

---

### **3. Key Kubernetes Objects**

```
Deployment
    â”œâ”€â”€ ReplicaSet
    â”‚   â”œâ”€â”€ Pod
    â”‚   â”œâ”€â”€ Pod
    â”‚   â””â”€â”€ Pod
    â””â”€â”€ (manages versions)

Service
    â””â”€â”€ (routes traffic to pods)

ConfigMap
    â””â”€â”€ (configuration data)

Secret
    â””â”€â”€ (sensitive data)

PersistentVolume
    â””â”€â”€ (storage)
```

**Explain:**
- **Pod**: "Smallest deployable unit - one or more containers"
- **Deployment**: "Manages rollouts, updates, scaling"
- **ReplicaSet**: "Ensures desired number of pod replicas"
- **Service**: "Stable network endpoint for pods"

---

### **4. Real Production Example** (from your experience)

*"At Birlasoft, when we deployed microservices to Kubernetes:*

1. *Developers push code to GitHub*
2. *CI/CD pipeline (Jenkins) builds Docker image*
3. *Image pushed to JFrog Artifactory*
4. *Jenkins updates deployment YAML with new image tag*
5. *kubectl apply triggers rolling update*
6. *Kubernetes gradually replaces old pods with new ones*
7. *If health checks fail, automatic rollback occurs*
8. *Dynatrace and Prometheus monitor the rollout*

*This gave us zero-downtime deployments and the ability to roll back instantly if issues occurred."*

---

### **5. Self-Healing Example**

```
Time: 10:00 AM
State: 3 pods running âœ…

Time: 10:15 AM
Event: Node1 crashes ğŸ’¥
Current: 1 pod running âŒ

Kubernetes Actions:
â”œâ”€ Node Controller detects node down
â”œâ”€ Marks node as NotReady
â”œâ”€ Deployment Controller sees pods missing
â”œâ”€ Creates new pods
â”œâ”€ Scheduler assigns to healthy nodes
â””â”€ Kubelet starts containers

Time: 10:17 AM
State: 3 pods running âœ…
```

**Explain:**
*"If a node crashes, Kubernetes automatically detects it, marks pods as failed, and reschedules them on healthy nodes. This happens without any manual intervention - it's part of Kubernetes' self-healing capability."*

---

### **6. Scaling Example**

```bash
# Scale up
kubectl scale deployment webapp --replicas=10

# What happens:
API Server â†’ etcd (update desired replicas to 10)
Deployment Controller â†’ sees 3 current, 10 desired
ReplicaSet Controller â†’ creates 7 new pods
Scheduler â†’ assigns to nodes
Kubelet â†’ starts containers

# Time: ~30 seconds for 7 new pods
```

---

## Common Interview Questions & Answers

### **Q: What happens when you create a deployment?**

**A:** 
*"When you run kubectl apply -f deployment.yaml:*
1. *kubectl sends request to API Server*
2. *API Server authenticates, authorizes, validates, and stores in etcd*
3. *Deployment Controller sees new deployment and creates a ReplicaSet*
4. *ReplicaSet Controller creates Pod objects*
5. *Scheduler assigns pods to nodes based on resources*
6. *Kubelet on each node pulls images and starts containers*
7. *kube-proxy sets up networking rules*
8. *Controllers continuously monitor and maintain desired state"*

---

### **Q: How does Kubernetes handle pod failures?**

**A:**
*"Kubernetes uses a reconciliation loop:*
1. *ReplicaSet Controller constantly checks: desired replicas vs current replicas*
2. *If pod crashes, current < desired*
3. *Controller immediately creates new pod to replace it*
4. *Scheduler assigns to a healthy node*
5. *Kubelet starts the container*
6. *Typically takes 10-30 seconds from crash to new pod running*

*I've seen this in production at Birlasoft - when pods crashed due to OOM errors, Kubernetes automatically restarted them while we investigated the root cause."*

---

### **Q: What's the difference between Deployment and ReplicaSet?**

**A:**
*"ReplicaSet just ensures N pod replicas are running. Deployment is higher-level:*
- *Manages ReplicaSets*
- *Handles rolling updates (creates new ReplicaSet, gradually shifts traffic)*
- *Maintains revision history for rollbacks*
- *Provides declarative update strategy*

*You typically never create ReplicaSets directly - you create Deployments, which create and manage ReplicaSets for you."*

---

### **Q: How does service discovery work?**

**A:**
*"Kubernetes has built-in DNS:*
1. *Every Service gets a DNS name: `service-name.namespace.svc.cluster.local`*
2. *Pods can reference services by name: `http://webapp-service`*
3. *kube-proxy creates iptables rules that load-balance traffic*
4. *When you call the service, traffic is distributed across all healthy pods*
5. *If pods are added/removed, kube-proxy automatically updates rules*

*This means applications don't need hardcoded IPs - they just use service names, and Kubernetes handles the routing."*

---

## Visual Summary for Interview

```
USER
 â”‚
 â”‚ kubectl apply -f deployment.yaml
 â–¼
API SERVER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º etcd (stores state)
 â”‚
 â”œâ”€â”€â–º Deployment Controller â”€â”€â–º Creates ReplicaSet
 â”‚
 â”œâ”€â”€â–º ReplicaSet Controller â”€â”€â–º Creates Pods
 â”‚
 â”œâ”€â”€â–º Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Assigns Pods to Nodes
 â”‚
 â–¼
NODES
 â”œâ”€ Kubelet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Pulls image, starts container
 â””â”€ Kube-proxy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Sets up networking

CONTINUOUS:
 â””â”€ Controllers monitor â”€â”€â”€â”€â”€â”€â–º Self-healing, scaling
```

---

This explanation covers everything you need for an interview. Start with the high-level answer, then go deeper based on their questions!